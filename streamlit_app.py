import streamlit as st
import pandas as pd
import numpy as np
import faiss
from opencc import OpenCC
import os
import openai

cc = OpenCC('s2twp')  # ç°¡é«”è½‰å°ç£ç¹é«”

# GPT é–‹é—œèˆ‡ API åˆå§‹åŒ–
enable_gpt = st.sidebar.checkbox("âœ… å•Ÿç”¨ GPT ç¿»è­¯å»ºè­°", value=False)
if enable_gpt:
    client = openai.OpenAI(api_key=st.secrets["openai_api_key"])

def gpt_translate(text):
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ç¿»è­¯åŠ©æ‰‹ï¼Œä½¿ç”¨å°ç£ç”¨èªã€‚"},
                {"role": "user", "content": f"è«‹å°‡ä»¥ä¸‹ç°¡é«”ä¸­æ–‡ç¿»è­¯æˆå°ç£å¸¸ç”¨çš„ç¹é«”ä¸­æ–‡ï¼š\n{text}"}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"GPT éŒ¯èª¤ï¼š{str(e)}"

# ----------- æ¨¡æ“¬ embedding å‡½æ•¸ ----------- #
def fake_embed(text):
    np.random.seed(abs(hash(text)) % (2**32))
    return np.random.rand(1536).astype("float32")

# ----------- åˆå§‹åŒ–ç¿»è­¯è¨˜æ†¶åº« ----------- #
data_path = "memory.csv"

if os.path.exists(data_path):
    df = pd.read_csv(data_path)
else:
    data = {
        "åŸæ–‡ï¼ˆç°¡é«”ï¼‰": [
            "ä»Šå¤©æ˜¯æ˜ŸæœŸäº”ã€‚",
            "ä»–æ­£åœ¨å­¸ç¿’ä¸­æ–‡ã€‚",
            "æˆ‘å–œæ­¡å–å’–å•¡ã€‚",
            "æˆ‘å€‘ä¸€èµ·å»å…¬åœ’æ•£æ­¥å§ã€‚",
            "é€™éƒ¨é›»å½±éå¸¸å¥½çœ‹ã€‚"
        ],
        "ç¿»è­¯ï¼ˆç¹é«”ï¼‰": [
            "ä»Šå¤©æ˜¯æ˜ŸæœŸäº”ã€‚",
            "ä»–æ­£åœ¨å­¸ç¿’ä¸­æ–‡ã€‚",
            "æˆ‘å–œæ­¡å–å’–å•¡ã€‚",
            "æˆ‘å€‘ä¸€èµ·å»å…¬åœ’æ•£æ­¥å§ã€‚",
            "é€™éƒ¨é›»å½±éå¸¸å¥½çœ‹ã€‚"
        ]
    }
    df = pd.DataFrame(data)
    df.to_csv(data_path, index=False)

# ----------- å»ºç«‹ FAISS å‘é‡åº« ----------- #
def rebuild_index():
    global index, df
    source_texts = df["åŸæ–‡ï¼ˆç°¡é«”ï¼‰"].tolist()
    embeddings = [fake_embed(text) for text in source_texts]
    index = faiss.IndexFlatL2(1536)
    index.add(np.array(embeddings))

rebuild_index()

# ----------- Streamlit UI ----------- #
st.title("ç¿»è­¯è¨˜æ†¶ ç¹é«”ç¿»è­¯å·¥å…·ï¼ˆå¯é¸ GPTï¼‰")
st.write("è«‹è¼¸å…¥ä¸€æ®µç°¡é«”ä¸­æ–‡ï¼Œç³»çµ±æœƒæ ¹æ“šéå¾€ç¿»è­¯è¨˜éŒ„ã€ç¹é«”è½‰æ›åŠ GPT æ¨¡å‹ç”¢å‡ºå»ºè­°ç¿»è­¯ï¼š")

user_input = st.text_input("è¼¸å…¥ç°¡é«”åŸæ–‡")

# è‡ªè¨‚è©å½™ä¿®æ­£
def apply_custom_dict(text):
    custom_dict = {
        "åˆå§‹é ­åƒ": "é è¨­é ­åƒ",
        "è³¬è™Ÿ": "å¸³è™Ÿ",
    }
    for k, v in custom_dict.items():
        text = text.replace(k, v)
    return text

if user_input:
    query_vector = fake_embed(user_input)
    D, I = index.search(np.array([query_vector]), k=1)
    best_idx = I[0][0]

    st.subheader("ç³»çµ±æ‰¾åˆ°çš„æœ€æ¥è¿‘ç¿»è­¯ç´€éŒ„ï¼š")
    suggested_translation = df.iloc[best_idx]['ç¿»è­¯ï¼ˆç¹é«”ï¼‰']
    st.write(f"ğŸ‘‰ {df.iloc[best_idx]['åŸæ–‡ï¼ˆç°¡é«”ï¼‰']} â†’ {suggested_translation}")

    # å˜—è©¦ç°¡é«”è½‰å°ç£ç¹é«”è½‰æ› + è©å½™ä¿®æ­£
    converted_text = cc.convert(user_input)
    converted_text = apply_custom_dict(converted_text)

    st.subheader("æœ€çµ‚ç¿»è­¯å»ºè­°ï¼š")
    st.success(converted_text)

    if enable_gpt:
        st.subheader("GPT ç¿»è­¯å»ºè­°ï¼š")
        gpt_result = gpt_translate(user_input)
        gpt_result = apply_custom_dict(gpt_result)
        st.info(gpt_result)

    if user_input not in df["åŸæ–‡ï¼ˆç°¡é«”ï¼‰"].values:
        df.loc[len(df)] = [user_input, converted_text]
        df.to_csv(data_path, index=False)
        st.info("âœ… æ­¤å¥å·²æ–°å¢è‡³ç¿»è­¯è¨˜æ†¶åº«")
        rebuild_index()

# ----------- æ‰¹æ¬¡ä¸Šå‚³ç¿»è­¯åŠŸèƒ½ ----------- #
st.header("ğŸ“„ æ‰¹æ¬¡ä¸Šå‚³ç°¡é«”æ–‡æœ¬é€²è¡Œç¿»è­¯")
uploaded_file = st.file_uploader("ä¸Šå‚³ Excel (.xlsx) æˆ–ç´”æ–‡å­—æª” (.txt)", type=["xlsx", "txt"])

if uploaded_file:
    if uploaded_file.name.endswith(".txt"):
        text_lines = uploaded_file.read().decode("utf-8").splitlines()
        batch_df = pd.DataFrame({"åŸæ–‡ï¼ˆç°¡é«”ï¼‰": [line for line in text_lines if line.strip()]})
    elif uploaded_file.name.endswith(".xlsx"):
        try:
            batch_df = pd.read_excel(uploaded_file)
        except ImportError:
            st.error("è«‹å…ˆå®‰è£ openpyxl å¥—ä»¶ï¼Œæˆ–æ”¹ç”¨ TXT æª”æ¡ˆã€‚")
            st.stop()

        if "åŸæ–‡ï¼ˆç°¡é«”ï¼‰" not in batch_df.columns:
            st.error("Excel æª”æ¡ˆéœ€åŒ…å«ã€åŸæ–‡ï¼ˆç°¡é«”ï¼‰ã€æ¬„ä½")
            st.stop()

    with st.spinner("æ­£åœ¨ç¿»è­¯..."):
        def memory_lookup(query):
            qv = fake_embed(query)
            _, idxs = index.search(np.array([qv]), k=1)
            base = df.iloc[idxs[0][0]]['ç¿»è­¯ï¼ˆç¹é«”ï¼‰']
            converted = cc.convert(query)
            return apply_custom_dict(converted)

        batch_df["ç¿»è­¯ï¼ˆç¹é«”ï¼‰"] = batch_df["åŸæ–‡ï¼ˆç°¡é«”ï¼‰"].apply(memory_lookup)

        st.success("ç¿»è­¯å®Œæˆï¼")
        st.dataframe(batch_df)

        csv_download = batch_df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ğŸ“¥ ä¸‹è¼‰ç¿»è­¯çµæœ (CSV)", csv_download, file_name="ç¿»è­¯çµæœ.csv", mime="text/csv")

# ----------- ä¸Šå‚³å­—åº«åŠŸèƒ½ ----------- #
st.header("ğŸ“š ä¸Šå‚³è‡ªå®šç¾©ç¿»è­¯è¨˜æ†¶åº«")
dict_file = st.file_uploader("ğŸ“¥ ä¸Šå‚³å­—åº« (éœ€ç‚ºåŒ…å«ã€åŸæ–‡ï¼ˆç°¡é«”ï¼‰ã€ã€ç¿»è­¯ï¼ˆç¹é«”ï¼‰ã€æ¬„ä½çš„ Excel æª”æ¡ˆ)", type="xlsx")
if dict_file:
    try:
        new_dict_df = pd.read_excel(dict_file)
        if "åŸæ–‡ï¼ˆç°¡é«”ï¼‰" in new_dict_df.columns and "ç¿»è­¯ï¼ˆç¹é«”ï¼‰" in new_dict_df.columns:
            before = len(df)
            df = pd.concat([df, new_dict_df], ignore_index=True).drop_duplicates(subset="åŸæ–‡ï¼ˆç°¡é«”ï¼‰")
            df.to_csv(data_path, index=False)
            rebuild_index()
            st.success(f"âœ… å­—åº«å·²æˆåŠŸåŒ¯å…¥ï¼Œå…±æ–°å¢ {len(df)-before} ç­†è³‡æ–™")
        else:
            st.error("âŒ æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ã€åŸæ–‡ï¼ˆç°¡é«”ï¼‰ã€æˆ–ã€ç¿»è­¯ï¼ˆç¹é«”ï¼‰ã€")
    except Exception as e:
        st.error(f"âŒ ä¸Šå‚³éŒ¯èª¤ï¼š{e}")
